%% BOOKS %%

@book{compint,
    title     = {Computational Intelligence},
    author    = {Engelbrecht, A.P.},
    isbn      = {9780470035610},
    year      = {2008},
    publisher = {John Wiley \& Sons},
	edition   = 2
}
%% END OF BOOKS %%

%% ARTICLES %%

@article{neuroevolution_review,
	author  = {Stanley, K.O. and Clune, J. and Lehman, J. and Miikkulainen, R.},
	title   = {Designing Neural Networks through Neuroevolution},
	journal = {Nat Mach Intell},
	volume  = {1},
	year    = {2019},
	pages   = {24-35},
	month   = jan,
	issn    = {1},
	doi     = {10.1038/s42256-018-0006-z},
	url     = {https://doi.org/10.1038/s42256-018-0006-z}
}

@phdthesis{neat_phd,
    title    = {Efficient Evolution of Neural Networks through Complexification},
    school   = {University of Texas},
    author   = {Stanley, K.O.},
    year     = {2004}
}

@article{neat_main,
	author = {Stanley, K.O. and Miikkulainen, R.},
	title = {Evolving Neural Networks through Augmenting Topologies},
	journal = {Evolutionary Computation},
	volume = {10},
	number = {2},
	pages = {99-127},
	year = {2002},
	month = mar,
	doi = {10.1162/106365602320169811},
	url = {https://doi.org/10.1162/106365602320169811}
}

@INPROCEEDINGS{neat_short,
  author={K. O. {Stanley} and R. {Miikkulainen}},
  booktitle={Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)},
  title={Efficient evolution of neural network topologies},
  year={2002},
  volume={2},
  number={},
  pages={1757-1762 vol.2},
  doi={10.1109/CEC.2002.1004508}
}


@article{universality_formal,
	author = {Cybenko, G.},
	title = {Approximation by superpositions of a sigmoidal function},
	journal = {Math. Control Signal Systems},
	volume = {2},
	number = {4},
	pages = {303-314},
	year = {1989},
	month   = dec,
	doi = {10.1007/BF02551274},
	url = {https://doi.org/10.1007/BF02551274}
}

@article{hyper_neat,
author = {Stanley, K. O. and D'Ambrosio, D. B. and Gauci, J.},
title = {A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks},
journal = {Artificial Life},
volume = {15},
number = {2},
pages = {185-212},
year = {2009},
doi = {10.1162/artl.2009.15.2.15202},
URL = {https://doi.org/10.1162/artl.2009.15.2.15202},
eprint = { https://doi.org/10.1162/artl.2009.15.2.15202}
}


@INPROCEEDINGS{mujoco,

  author={E. {Todorov} and T. {Erez} and Y. {Tassa}},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title={MuJoCo: A physics engine for model-based control},
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}
}

@inproceedings{ns_study,
author = {Gomes, J. and Mariano, P. and Christensen, A.},
title = {Devising Effective Novelty Search Algorithms: A Comprehensive Empirical Study},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/2739480.2754736},
doi = {10.1145/2739480.2754736},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {943–950},
numpages = {8},
keywords = {evolutionary robotics, premature convergence, novelty search, empirical study, neuroevolution},
location = {Madrid, Spain},
series = {GECCO '15}
}

@InProceedings{novelty_not_enough,
	author="Cuccu, G.
	and Faustino, G.",
	title="When Novelty Is Not Enough",
	booktitle="Applications of Evolutionary Computation",
	year="2011",
	publisher="Springer Berlin Heidelberg",
	pages="234--243",
	abstract="The idea of evolving novel rather than fit solutions has recently been offered as a way to automatically discover the kind of complex solutions that exhibit truly intelligent behavior. So far, novelty search has only been studied in the context of problems where the number of possible ``different'' solutions has been limited. In this paper, we show, using a task with a much larger solution space, that selecting for novelty alone does not offer an advantage over fitness-based selection. In addition, we examine how the idea of novelty search can be used to sustain diversity and improve the performance of standard, fitness-based search.",
	isbn="978-3-642-20525-5"
}

@InProceedings{multi_ns,
	author="Mouret, J.-B.",
	title="Novelty-Based Multiobjectivization",
	booktitle="New Horizons in Evolutionary Robotics",
	year="2011",
	publisher="Springer Berlin Heidelberg",
	pages="139--154",
	abstract="Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Pareto-based multi-objective evolutionary algorithmis employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant ``Novelty + Fitness'' is better at fine-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.",
	isbn="978-3-642-18272-3"
}

@inproceedings{minimal_ns,
	author = {Lehman, J. and Stanley, K.O.},
	title = {Revising the Evolutionary Computation Abstraction: Minimal Criteria Novelty Search},
	year = {2010},
	isbn = {9781450300728},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/1830483.1830503},
	doi = {10.1145/1830483.1830503},
	abstract = {Though based on abstractions of nature, current evolutionary algorithms and artificial life models lack the drive to complexity characteristic of natural evolution. Thus this paper argues that the prevalent fitness-pressure-based abstraction does not capture how natural evolution discovers complexity. Alternatively, this paper proposes that natural evolution can be abstracted as a process that discovers many ways to express the same functionality. That is, all successful organisms must meet the same minimal criteria of survival and reproduction. This abstraction leads to the key idea in this paper: Searching for novel ways of meeting the same minimal criteria, which is an accelerated model of this new abstraction, may be an effective search algorithm. Thus the existing novelty search method, which rewards any new behavior, is extended to enforce minimal criteria. Such minimal criteria novelty search prunes the space of viable behaviors and may often be more efficient than the search for novelty alone. In fact, when compared to the raw search for novelty and traditional fitness-based search in the two maze navigation experiments in this paper, minimal criteria novelty search evolves solutions more consistently. It is possible that refining the evolutionary computation abstraction in this way may lead to solving more ambitious problems and evolving more complex artificial organisms.},
	booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
	pages = {103–110},
	numpages = {8},
	series = {GECCO '10}
}


%
%% END OF ARTICLES %%

%% THESISES %%

%% END OF THESISES %%

%% MISC/WEBSITES/ETC %%

% NOTE: do NOT forget to add a accessed-by date!
@misc{universality_informal,
	author = {Nielsen, M.},
	title = {{A visual proof that neural nets can compute any function}},
	howpublished = "\url{http://neuralnetworksanddeeplearning.com/chap4.html}",
	year = {2019},
	note = {Accessed: 2020-10-22}
}
