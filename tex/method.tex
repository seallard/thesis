\section{Method}
Variations on novelty search are often studied in the domain of maze navigation. Mazes are useful
since they allow for the construction of tasks of varying deceptiveness. If the objective function is
based on the distance to the end of the maze, simply following its gradient might trap the agent in
dead ends. The maze can be seen as an abstraction for difficult problems in which the fitness
landscape is deceptive.

\subsection{Maze navigation task}
In maze navigation, a simulated robot must solve a maze within a limited number of time steps.
The robot is controlled by a neural network which, given the readings from the robots sensors,
outputs two forces adjusting the robots linear and angular velocity.

The robot has six rangefinder sensors which measure the distance to walls. If a wall occurs within the sensors
direction and maximum range, the distance is returned. It also has four radar sensors which return binary
values indicating whether the end point is directly reachable within their field of view.

Three mazes are used to compare the novelty search variants (see Figure \ref{mazes}) similar to the ones in \cite{ns_study, novelty_alone}.
The first one is easy, only requiring the robot to slightly shift its path to reach the end point. The second is more
difficult, with deeper dead ends that require the robots to explore parts of the maze of much lower fitness.
The last maze is open at the top, the robots are free to move upwards from the actual maze.
$95 \%$ of the maze area is irrelevant to actually solving it. This creates a
large behaviour space which decouples novelty from utility, making novelty search alone insufficient
to solve it.

Any robot which navigates within 5 units of the end point is classified as having solved the maze. The robots
were given $400$ time steps to solve the maze, a limit which was set as to force them to navigate the maze
in a direct manner.


\begin{figure}[H]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{mdframed}
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \scalebox{0.25}{\input{resources/mazes/medium.pgf}}
            \caption{Medium.}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \scalebox{0.25}{\input{resources/mazes/hard.pgf}}
            \caption{Hard.}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
            \centering
            \scalebox{0.25}{\input{resources/mazes/open.pgf}}
            \caption{Open.}
        \end{subfigure}
    \end{mdframed}
    \caption{The robot must navigate from the green point to the red in order to solve the maze.}
    \label{mazes}
\end{figure}


\subsection{Fitness and novelty metric}
\label{subsection:metrics}
The fitness function is defined as $f = b - d$ where $b$ is a constant ensuring that the fitness stays positive
and $d$ is the euclidean distance from the robot's final position to the maze end point after an evaluation.
The constant $b$ was set to the length of the diagonal of the mazes.

The distance measure used in the novelty metric, see Equation 1, is defined as the euclidean distance between the
final positions of the robots in the maze.

These metrics have consistently been used in previous studies on novelty search, see for example \cite{ns_study,novelty_alone}.

\subsection{Proposed combinations}
\label{subsection:linearisation}
The idea behind both of the studied combinations is to use novelty search in order to introduce diversity
when the search algorithm prematurely converges to local optima.

As shown in \cite{novelty_not_enough}, a weighted sum of novelty and fitness can be used to score each solution
\[
    score(i) = (1-\rho) \cdot \overline{fitness}(i) + \rho \cdot \overline{novelty}(i)
\]
where $\rho \in [0,1]$ and the fitness and novelty scores are normalised according to
\begin{align*}
    \overline{fitness}(i) =  \frac{fit(i) - fit_{min}}{fit_{max} - fit_{min}} && \overline{novelty}(i) =  \frac{novelty(i) - novelty_{min}}{novelty_{max} - novelty_{min}}
\end{align*}
where the maximum and minimum are the extreme values observed in the current population for each respective metric.
The best results were obtained with $\rho$ in $[0.4,0.9]$. In the discussion of \cite{novelty_not_enough}, it is suggested
that the $\rho$ parameter could be updated based on the performance of the search algorithm.

\subsubsection{Dynamic linearisation}
\label{subsubsection:dynamic_linearisation}
I propose the following method for updating $\rho$
\begin{align*}
    \rho(n) =
        \begin{cases}
            0.1 + \frac{n}{n_{max}} \cdot 0.8 & \text{if $n \leq n_{max}$}\\
            0.9 & \text{otherwise}
        \end{cases}
\end{align*}
where $n$ is the number of generations since a solution of higher fitness was found and
$n_{max}$ is a threshold number of generations at which $\rho$ reaches a maximum
of $0.9$. For the experiments, $n_{max}=15$ was used.

\subsubsection{Novelty injection}
\label{subsection:injection}
In novelty injection, the novelty metric is used only if stagnation is detected. That is, if no solution is found with a
higher fitness within some threshold number of generations $n_{max}$, the fitness metric is swapped for novelty for a fixed
number of generations $g_{swap}$.
\begin{align*}
    \rho(n) =
        \begin{cases}
            0 & \text{if $n \leq n_{max}$}\\
            1 & \text{otherwise}
        \end{cases}
\end{align*}
In the experiments, $n_{max}=15$ and $g_{swap} = 15$ was used. These constants were not determined by experimentation due to
limited access to computational time. They were simply selected because they were able to solve the easy maze. I assumed that if novelty injection
is a good method, its performance should be robust to variations in those parameters. See section 7 for a further discussion.

\subsection{Experimental design}
\label{subsection:design}
Each variant is evaluated over $30$ runs for all of the mazes, each run being terminated once an ANN
being able to solve the maze is found or $100 000$ evaluations have been performed. For the open maze, the number of
evaluations was increased to $140 000$ and $550$ time steps.
To provide a baseline comparison, the experiment is run with pure fitness and novelty. To see how
the proposed combinations compare to existing ones, it is repeated with a fixed 50-50 weighted sum
of fitness and novelty which has been shown to perform well on the maze task \cite{ns_study}.
The parameters used by the genetic algorithm can be found in Appendix A.