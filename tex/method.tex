\section{Method}
Variations on novelty search are often studied in the domain of maze navigation. Mazes are useful
since they allow for the construction of tasks of varying deceptiveness. If the objective function is
based on the distance to the end of the maze, simply following its gradient might trap the agent in
dead-ends. The maze can be seen as an abstraction for difficult problems in which the fitness
landscape is deceptive.

\subsection{Maze navigation task}
In maze navigation, a simulated robot must solve a maze within a limited number of time steps.
The robot is controlled by a neural network which, given the readings from the robots sensors,
outputs two forces adjusting the robots linear and angular velocity.

The robot has six rangefinder sensors which measure the distance to walls. If a wall occurs within the sensors
direction and maximum range, the distance is returned. It also has four radar sensors which return binary
values indicating whether the end-point is directly reachable within their field of view.

Two mazes are used to compare the novelty search variants (see Figure \ref{mazes}) similar to the ones in \cite{novelty_alone}.
Any robot which navigates within 5 units of the end point is classified as having solved the maze. The robots
were given $400$ time steps to solve the maze, a limit which was set as to force the robots to navigate the maze


\begin{figure}[H]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{mdframed}
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \hspace*{2em}\scalebox{0.3}{\input{resources/mazes/medium.pgf}}
            \caption{Medium.}
        \end{subfigure}
        \begin{subfigure}[b]{0.5\textwidth}
            \centering
            \hspace*{5em}\scalebox{0.3}{\input{resources/mazes/hard.pgf}}
            \caption{Hard.}
        \end{subfigure}
    \end{mdframed}
    \caption{The robot must navigate from the green point to the red in order to solve the maze.
             (b) is more deceptive than (a).}
    \label{mazes}
\end{figure}

\subsection{Fitness and novelty metric}
The fitness function is defined as $f = b - d$ where $b$ is a constant ensuring the fitness stays positive
and $d$ is the distance from the robots final position to the maze end-point after an evaluation.
The novelty metric is defined as the euclidean distance between the robots end-positions in the maze.
These metrics have consistently been used in previous studies on novelty search, see for example \cite{ns_study,novelty_alone}.

\subsection{Proposed combinations}
The idea behind both of the studied combinations is to use novelty search in order to introduce diversity
when the search algorithm prematurely converges on local optima.

\subsubsection{Dynamic linearisation of novelty and fitness}
As shown in \cite{novelty_not_enough}, a weighted sum of novelty and fitness can be used to score each solution
\[
    score(i) = (1-\rho) \cdot \overline{fitness}(i) + \rho \cdot \overline{novelty}(i)
\]
where $\rho \in [0,1]$ and the fitness and novelty score are normalised according to
\begin{align*}
    \overline{fitness}(i) =  \frac{fit(i) - fit_{min}}{fit_{max} - fit_{min}} && \overline{novelty}(i) =  \frac{novelty(i) - novelty_{min}}{novelty_{max} - novelty_{min}}
\end{align*}
\todo{Explain what min and max is used. What is the point of normalising like this?}
The best results were obtained with $\rho=0.4-0.9$. In the discussion of \cite{novelty_not_enough}, it is suggested
that the $\rho$ parameter could be updated based on the performance of the search algorithm. I propose the following
method for updating $\rho$
\begin{align*}
    \rho(n) =
        \begin{cases}
            0.1 + \frac{n}{n_{max}} \cdot 0.8 & \text{if $n$ < $n_{max}$}\\
            0.9 & \text{otherwise}
        \end{cases}
\end{align*}
where $n$ is the number of generations since a better solution was found and
$n_{max}$ is a threshold number of generations at which $\rho$ reaches a maximum
of $0.9$. For the experiments, $n_{max}=15$ was used.

\subsubsection{Novelty injection}
In novelty injection, the novelty metric is used only if stagnation is detected. That is, if no solution is found with a
higher fitness within some threshold number of generations $n_{max}$ the fitness metric is swapped for novelty for a fixed
number of generations.
\begin{align*}
    \rho(n) =
        \begin{cases}
            0 & \text{if $n$ < $n_{max}$}\\
            1 & \text{otherwise}
        \end{cases}
\end{align*}
In the experiments, the novelty metric was used for $10$ generations.

\subsection{Experimental design}
Each variant is evaluated over $30$ runs for both mazes, each run being terminated once an ANN
able to solve the maze is found or $100 000$ evaluations is reached.
To provide a baseline comparison, the experiment is run with pure fitness and novelty. To see how
the proposed combinations compare to existing ones, it is repeated with a fixed 50-50 weighted sum
of fitness and novelty which has been shown to perform well on the maze task \cite{ns_study}.
